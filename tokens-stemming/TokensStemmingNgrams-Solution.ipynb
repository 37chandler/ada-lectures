{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Questions:\n",
    "\n",
    "1. Find emojis in the chat corpus.\n",
    "\n",
    "1. Determine a normalization scheme. (What needs to be normalized, how would you do it?)\n",
    "\n",
    "1. Count the happy vs sad emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chat = text5 # give it a nice name. \n",
    "\n",
    "# Let's find emojis in chat. \n",
    "potential_emojis = {w for w in chat if \":\" in w or \";\" in w or \"=\" in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!=',\n",
       " '.:',\n",
       " '.;)',\n",
       " '//www.wunderground.com/cgi-bin/findweather/getForecast?query=95953#FIR',\n",
       " '10:49',\n",
       " '2:55',\n",
       " '3:45',\n",
       " '4:03',\n",
       " '6:38',\n",
       " '6:41',\n",
       " '6:51',\n",
       " '6:53',\n",
       " '7:45',\n",
       " '9:10',\n",
       " ':',\n",
       " ':(',\n",
       " ':)',\n",
       " ':):):)',\n",
       " ':-(',\n",
       " ':-)',\n",
       " ':-@',\n",
       " ':-o',\n",
       " ':.',\n",
       " ':/',\n",
       " ':@',\n",
       " ':D',\n",
       " ':O',\n",
       " ':P',\n",
       " ':]',\n",
       " ':beer:',\n",
       " ':blush:',\n",
       " ':love:',\n",
       " ':o *',\n",
       " ':p',\n",
       " ':tongue:',\n",
       " ':|',\n",
       " ';',\n",
       " '; ..',\n",
       " ';)',\n",
       " ';-(',\n",
       " ';-)',\n",
       " ';0',\n",
       " ';]',\n",
       " ';p',\n",
       " '=',\n",
       " \"='s\",\n",
       " '=(',\n",
       " '=)',\n",
       " '=-\\\\',\n",
       " '=/',\n",
       " '=D',\n",
       " '=O',\n",
       " '=[',\n",
       " '=]',\n",
       " '=p',\n",
       " '>:->',\n",
       " ']:)',\n",
       " 'capab;e',\n",
       " 'd=',\n",
       " 'http://forums.talkcity.com/tc-adults/start ',\n",
       " 'http://www.shadowbots.com',\n",
       " 'n;t',\n",
       " 'o<|=D'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potential_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we're catching some non-emojis, but let's assume we're getting most of the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':(',\n",
       " ':)',\n",
       " ':-(',\n",
       " ':-)',\n",
       " ':-@',\n",
       " ':-o',\n",
       " ':.',\n",
       " ':/',\n",
       " ':@',\n",
       " ':D',\n",
       " ':O',\n",
       " ':P',\n",
       " ':]',\n",
       " ':p',\n",
       " ':|',\n",
       " ';)',\n",
       " ';-(',\n",
       " ';-)',\n",
       " ';0',\n",
       " ';]',\n",
       " ';p',\n",
       " '=(',\n",
       " '=)',\n",
       " '=-\\\\',\n",
       " '=/',\n",
       " '=D',\n",
       " '=O',\n",
       " '=[',\n",
       " '=]',\n",
       " '=p']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are all oriented left-to-right, so let's make a regex to find them. \n",
    "emoji = re.compile(r\"^[:;=]-?[)(\\]PD@op|O]$\") # misses '>:->' and ']:)' and repeats. Insert shruggie\n",
    "emoji2 = re.compile(r\"^[:;=]-?.$\")\n",
    "emojis = {w for w in chat if emoji2.search(w)}\n",
    "sorted(emojis)\n",
    "#len(emojis)\n",
    "# could normalize by removing hyphens, case letters to upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# Count happy vs sad\n",
    "happy = [w for w in chat if w in {\":-)\",\":)\",\":D\",\";-)\",\"=)\"}]\n",
    "sad = [w for w in chat if w in {\":-(\",\":(\",\";-(\",\"=(\"}]\n",
    "\n",
    "print(len(happy))\n",
    "print(len(sad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stemming\n",
    "\n",
    "Let's go through some stemming examples from the NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vowels = re.compile(r'[aeiouyAEIOU]')\n",
    "\n",
    "len({w for w in nltk.corpus.words.words() if not vowels.search(w[:-3]) and w[-3:] == \"ing\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aid of that Almighty Power which has hitherto protected me and enabled me to bring to favorable issues other important but still greatly inferior trusts heretofore confided to me by my country . The broad foundation upon which our Constitution rests being the people -- a breath of theirs having made , as a breath can unmake , change , or modify it -- it can be assigned to none of the great divisions of government but to that of democracy . If such is its theory , those who are called upon to administer it must recognize as its\n",
      "\n",
      "\n",
      "\n",
      "aid of that almighti power which ha hitherto protect me and enabl me to bring to favor issu other import but still greatli inferior trust heretofor confid to me by my countri . the broad foundat upon which our constitut rest be the peopl -- a breath of their have made , as a breath can unmak , chang , or modifi it -- it can be assign to none of the great divis of govern but to that of democraci . If such is it theori , those who are call upon to administ it must recogn as it\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer() # give it a short name.\n",
    "start = 30000\n",
    "distance = 100\n",
    "\n",
    "print(\" \".join(text4[start:(start + distance)]))\n",
    "print(\"\\n\\n\")\n",
    "print(\" \".join([porter.stem(w) for w in text4[start:(start + distance)]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9754\n"
     ]
    }
   ],
   "source": [
    "# words in inaugural addresses\n",
    "print(len(set(text4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5470\n",
      "1.783180987202925\n"
     ]
    }
   ],
   "source": [
    "inaug_stemmed = {porter.stem(w.lower()) for w in text4}\n",
    "\n",
    "print(len(inaug_stemmed))\n",
    "\n",
    "print(len(set(text4))/len(inaug_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Language Models\n",
    "Let's find some common n-grams in S&S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = FreqDist(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01443041193422614"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.freq('a') # what does this mean? It's the number of times \"a\" is used in \"Sense & Sensibility\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " 'couldn',\n",
       " 'didn',\n",
       " 'doesn',\n",
       " 'hadn',\n",
       " 'hasn',\n",
       " 'haven',\n",
       " 'isn',\n",
       " 'ma',\n",
       " 'mightn',\n",
       " 'mustn',\n",
       " 'needn',\n",
       " 'shan',\n",
       " 'shouldn',\n",
       " 'wasn',\n",
       " 'weren',\n",
       " 'won',\n",
       " 'wouldn']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"hAPPY \".isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elinor', 685),\n",
       " ('could', 578),\n",
       " ('marianne', 566),\n",
       " ('mrs', 530),\n",
       " ('would', 515),\n",
       " ('said', 397),\n",
       " ('every', 377),\n",
       " ('one', 331),\n",
       " ('much', 290),\n",
       " ('must', 283)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sw = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elinor : 685 : 0.012688474789760307\n",
      "could : 578 : 0.010706479457637166\n",
      "marianne : 566 : 0.010484199607305598\n",
      "mrs : 530 : 0.009817360056310896\n",
      "would : 515 : 0.009539510243396436\n",
      "said : 397 : 0.0073537583818026895\n",
      "every : 377 : 0.0069832919645834105\n",
      "one : 331 : 0.006131219204979069\n",
      "much : 290 : 0.005371763049679547\n",
      "must : 283 : 0.005242099803652799\n",
      "sister : 282 : 0.005223576482791835\n",
      "edward : 263 : 0.00487163338643352\n",
      "mother : 258 : 0.0047790167821287\n",
      "dashwood : 252 : 0.004667876856962916\n",
      "well : 240 : 0.004445597006631349\n",
      "time : 239 : 0.004427073685770385\n",
      "know : 232 : 0.004297410439743637\n",
      "jennings : 230 : 0.004260363798021709\n",
      "though : 216 : 0.004001037305968214\n",
      "willoughby : 216 : 0.004001037305968214\n",
      "elinor : 685 : 0.012688474789760307\n",
      "could : 578 : 0.010706479457637166\n",
      "marianne : 566 : 0.010484199607305598\n",
      "mrs : 530 : 0.009817360056310896\n",
      "would : 515 : 0.009539510243396436\n",
      "said : 397 : 0.0073537583818026895\n",
      "every : 377 : 0.0069832919645834105\n",
      "one : 331 : 0.006131219204979069\n",
      "much : 290 : 0.005371763049679547\n",
      "must : 283 : 0.005242099803652799\n",
      "sister : 282 : 0.005223576482791835\n",
      "edward : 263 : 0.00487163338643352\n",
      "mother : 258 : 0.0047790167821287\n",
      "dashwood : 252 : 0.004667876856962916\n",
      "well : 240 : 0.004445597006631349\n",
      "time : 239 : 0.004427073685770385\n",
      "know : 232 : 0.004297410439743637\n",
      "jennings : 230 : 0.004260363798021709\n",
      "though : 216 : 0.004001037305968214\n",
      "willoughby : 216 : 0.004001037305968214\n",
      "elinor : 685 : 0.012688474789760307\n",
      "could : 578 : 0.010706479457637166\n",
      "marianne : 566 : 0.010484199607305598\n",
      "mrs : 530 : 0.009817360056310896\n",
      "would : 515 : 0.009539510243396436\n",
      "said : 397 : 0.0073537583818026895\n",
      "every : 377 : 0.0069832919645834105\n",
      "one : 331 : 0.006131219204979069\n",
      "much : 290 : 0.005371763049679547\n",
      "must : 283 : 0.005242099803652799\n",
      "sister : 282 : 0.005223576482791835\n",
      "edward : 263 : 0.00487163338643352\n",
      "mother : 258 : 0.0047790167821287\n",
      "dashwood : 252 : 0.004667876856962916\n",
      "well : 240 : 0.004445597006631349\n",
      "time : 239 : 0.004427073685770385\n",
      "know : 232 : 0.004297410439743637\n",
      "jennings : 230 : 0.004260363798021709\n",
      "though : 216 : 0.004001037305968214\n",
      "willoughby : 216 : 0.004001037305968214\n",
      "elinor : 685 : 0.012688474789760307\n",
      "could : 578 : 0.010706479457637166\n",
      "marianne : 566 : 0.010484199607305598\n",
      "mrs : 530 : 0.009817360056310896\n",
      "would : 515 : 0.009539510243396436\n",
      "said : 397 : 0.0073537583818026895\n",
      "every : 377 : 0.0069832919645834105\n",
      "one : 331 : 0.006131219204979069\n",
      "much : 290 : 0.005371763049679547\n",
      "must : 283 : 0.005242099803652799\n",
      "sister : 282 : 0.005223576482791835\n",
      "edward : 263 : 0.00487163338643352\n",
      "mother : 258 : 0.0047790167821287\n",
      "dashwood : 252 : 0.004667876856962916\n",
      "well : 240 : 0.004445597006631349\n",
      "time : 239 : 0.004427073685770385\n",
      "know : 232 : 0.004297410439743637\n",
      "jennings : 230 : 0.004260363798021709\n",
      "though : 216 : 0.004001037305968214\n",
      "willoughby : 216 : 0.004001037305968214\n",
      "elinor : 685 : 0.012688474789760307\n",
      "could : 578 : 0.010706479457637166\n",
      "marianne : 566 : 0.010484199607305598\n",
      "mrs : 530 : 0.009817360056310896\n",
      "would : 515 : 0.009539510243396436\n",
      "said : 397 : 0.0073537583818026895\n",
      "every : 377 : 0.0069832919645834105\n",
      "one : 331 : 0.006131219204979069\n",
      "much : 290 : 0.005371763049679547\n",
      "must : 283 : 0.005242099803652799\n",
      "sister : 282 : 0.005223576482791835\n",
      "edward : 263 : 0.00487163338643352\n",
      "mother : 258 : 0.0047790167821287\n",
      "dashwood : 252 : 0.004667876856962916\n",
      "well : 240 : 0.004445597006631349\n",
      "time : 239 : 0.004427073685770385\n",
      "know : 232 : 0.004297410439743637\n",
      "jennings : 230 : 0.004260363798021709\n",
      "though : 216 : 0.004001037305968214\n",
      "willoughby : 216 : 0.004001037305968214\n",
      "elinor : 685 : 0.012688474789760307\n",
      "could : 578 : 0.010706479457637166\n",
      "marianne : 566 : 0.010484199607305598\n",
      "mrs : 530 : 0.009817360056310896\n",
      "would : 515 : 0.009539510243396436\n",
      "said : 397 : 0.0073537583818026895\n",
      "every : 377 : 0.0069832919645834105\n",
      "one : 331 : 0.006131219204979069\n",
      "much : 290 : 0.005371763049679547\n",
      "must : 283 : 0.005242099803652799\n",
      "sister : 282 : 0.005223576482791835\n",
      "edward : 263 : 0.00487163338643352\n",
      "mother : 258 : 0.0047790167821287\n",
      "dashwood : 252 : 0.004667876856962916\n",
      "well : 240 : 0.004445597006631349\n",
      "time : 239 : 0.004427073685770385\n",
      "know : 232 : 0.004297410439743637\n",
      "jennings : 230 : 0.004260363798021709\n",
      "though : 216 : 0.004001037305968214\n",
      "willoughby : 216 : 0.004001037305968214\n",
      "elinor : 685 : 0.012688474789760307\n",
      "could : 578 : 0.010706479457637166\n",
      "marianne : 566 : 0.010484199607305598\n",
      "mrs : 530 : 0.009817360056310896\n",
      "would : 515 : 0.009539510243396436\n",
      "said : 397 : 0.0073537583818026895\n",
      "every : 377 : 0.0069832919645834105\n",
      "one : 331 : 0.006131219204979069\n",
      "much : 290 : 0.005371763049679547\n",
      "must : 283 : 0.005242099803652799\n",
      "sister : 282 : 0.005223576482791835\n",
      "edward : 263 : 0.00487163338643352\n",
      "mother : 258 : 0.0047790167821287\n",
      "dashwood : 252 : 0.004667876856962916\n",
      "well : 240 : 0.004445597006631349\n",
      "time : 239 : 0.004427073685770385\n",
      "know : 232 : 0.004297410439743637\n",
      "jennings : 230 : 0.004260363798021709\n",
      "though : 216 : 0.004001037305968214\n",
      "willoughby : 216 : 0.004001037305968214\n",
      "elinor : 685 : 0.012688474789760307\n",
      "could : 578 : 0.010706479457637166\n",
      "marianne : 566 : 0.010484199607305598\n",
      "mrs : 530 : 0.009817360056310896\n",
      "would : 515 : 0.009539510243396436\n",
      "said : 397 : 0.0073537583818026895\n",
      "every : 377 : 0.0069832919645834105\n",
      "one : 331 : 0.006131219204979069\n",
      "much : 290 : 0.005371763049679547\n",
      "must : 283 : 0.005242099803652799\n",
      "sister : 282 : 0.005223576482791835\n",
      "edward : 263 : 0.00487163338643352\n",
      "mother : 258 : 0.0047790167821287\n",
      "dashwood : 252 : 0.004667876856962916\n",
      "well : 240 : 0.004445597006631349\n",
      "time : 239 : 0.004427073685770385\n",
      "know : 232 : 0.004297410439743637\n",
      "jennings : 230 : 0.004260363798021709\n",
      "though : 216 : 0.004001037305968214\n",
      "willoughby : 216 : 0.004001037305968214\n",
      "38.3 s ± 977 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "fd = FreqDist([w.lower() for w in text2\n",
    "               if w.lower() not in nltk.corpus.stopwords.words(\"english\") \n",
    "               and w.isalpha()])\n",
    "\n",
    "#fd = FreqDist([w.lower() for w in text1]) \n",
    "#               if w.lower() not in \n",
    "#               nltk.corpus.stopwords.words(\"english\") \n",
    "#               and w.isalpha()])\n",
    "\n",
    "\n",
    "total_words = sum([count for word, count in fd.items()])\n",
    "\n",
    "for pairs in fd.most_common(20) :\n",
    "    print(\" : \".join([pairs[0],str(pairs[1]),str(pairs[1]/total_words)]))\n",
    "    \n",
    "#4063/3861"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'is', 'a', 'sentence', 'that', 'i', 'am', 'writing', 'in', 'applied', 'data', 'analytics']\n"
     ]
    }
   ],
   "source": [
    "words=\"here is a sentence that i am writing in applied data analytics\".split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentence', 'writing', 'applied', 'data', 'analytics']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in words if w not in nltk.corpus.stopwords.words(\"english\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'analytics'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'that',\n",
       " 'i',\n",
       " 'am',\n",
       " 'writing',\n",
       " 'in',\n",
       " 'applied',\n",
       " 'data']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sw = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "sw.remove('her')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doing',\n",
       " 'don',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " 'has',\n",
       " 'hasn',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'having',\n",
       " 'he',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " 'she',\n",
       " 'should',\n",
       " 'shouldn',\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " 'wouldn',\n",
       " 'y',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = FreqDist([\" \".join(b) for b in nltk.ngrams(text2,3) if b[0] == \"I\" and b[1] == \"am\"]) # could use bigram function instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I am sure', 72),\n",
       " ('I am not', 12),\n",
       " ('I am afraid', 11),\n",
       " ('I am so', 11),\n",
       " ('I am sorry', 11),\n",
       " ('I am very', 10),\n",
       " ('I am glad', 4),\n",
       " ('I am now', 4),\n",
       " ('I am monstrous', 4),\n",
       " ('I am convinced', 3)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am sure: 72\n",
      "I am convinced: 3\n",
      "I am afraid: 11\n",
      "I am by: 1\n",
      "I am very: 10\n",
      "I am before: 1\n",
      "I am glad: 4\n",
      "I am able: 1\n",
      "I am ready: 1\n",
      "I am not: 12\n",
      "I am much: 3\n",
      "I am with: 1\n",
      "I am almost: 2\n",
      "I am particularly: 2\n",
      "I am the: 2\n",
      "I am heartily: 1\n",
      "I am ,\": 1\n",
      "I am now: 4\n",
      "I am unable: 1\n",
      "I am persuaded: 1\n",
      "I am happy: 1\n",
      "I am perfectly: 3\n",
      "I am guilty: 1\n",
      "I am so: 11\n",
      "I am only: 2\n",
      "I am going: 1\n",
      "I am ,: 3\n",
      "I am confined: 1\n",
      "I am monstrous: 4\n",
      "I am flattered: 1\n",
      "I am .: 1\n",
      "I am always: 3\n",
      "I am at: 1\n",
      "I am ;: 1\n",
      "I am sorry: 11\n",
      "I am a: 1\n",
      "I am determined: 1\n",
      "I am in: 3\n",
      "I am alive: 2\n",
      "I am resolved: 1\n",
      "I am rather: 1\n",
      "I am bound: 1\n",
      "I am capable: 2\n",
      "I am delighted: 1\n",
      "I am ever: 1\n",
      "I am writing: 1\n",
      "I am NOT: 1\n",
      "I am commissioned: 1\n",
      "I am quite: 2\n",
      "I am miserable: 1\n",
      "I am to: 2\n",
      "I am wretched: 1\n",
      "I am justified: 1\n",
      "I am right: 1\n",
      "I am well: 2\n",
      "I am amazingly: 1\n",
      "I am come: 1\n",
      "I am extremely: 2\n",
      "I am excessively: 1\n",
      "I am informed: 1\n",
      "I am charged: 1\n",
      "I am no: 1\n",
      "I am talking: 1\n",
      "I am ruined: 1\n",
      "I am shut: 1\n",
      "I am allowed: 1\n",
      "I am thankful: 1\n",
      "I am doing: 1\n",
      "I am grown: 2\n",
      "I am disappointed: 1\n"
     ]
    }
   ],
   "source": [
    "for gram, count in fd.items() :\n",
    "    if gram[0] == \"I\" :\n",
    "        print(gram + \": \" + str(count))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_words = sum([count for pair, count in fd.items() if pair[0] == \"I\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for gram, count in fd.most_common(30) :\n",
    "    if gram[0] == \"I\" : \n",
    "        print(gram)\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for gram,count in sorted(fd.items(), key=lambda pair: pair[1], reverse=True) : \n",
    "    if gram[0] == \"I\" :\n",
    "        print(\" : \".join([str(gram),str(count),str(round(count/total_words,3))]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd = FreqDist(nltk.ngrams(text2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words = 0\n",
    "\n",
    "for gram,count in sorted(fd.items(), key=lambda pair: pair[1], reverse=True) : \n",
    "    if gram[0] == \"I\" and gram[1] == \"am\" :\n",
    "        total_words += count\n",
    "        print(\" : \".join([str(gram),str(count)])) \n",
    "        \n",
    "\n",
    "print(72/total_words)\n",
    "print(12/total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matches\n"
     ]
    }
   ],
   "source": [
    "text5.concordance(\"k fed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## N-gram models\n",
    "\n",
    "Let's make a function that takes in text, builds a freq dist and generates text with various n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def weighted_choice(freq_dist):\n",
    "    weight_total = sum([count for token,count in freq_dist.items()])\n",
    "    n = random.uniform(0, weight_total)\n",
    "    for token, count in freq_dist.items() :\n",
    "        if n < count:\n",
    "            return(token)\n",
    "        n = n - count\n",
    "    return(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = {'whale':600,'ishmael':100,'ahab':20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ishmael'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_choice(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weighted_choice(FreqDist(text5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_unigram(text,length=10) :\n",
    "    fd = FreqDist(text)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(length) :\n",
    "        results.append(weighted_choice(fd))\n",
    "        \n",
    "    return(\" \".join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of proper it ? with much nothing to light s'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_unigram(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'could too to was , Elinor house , in But'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_unigram(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':-) in - im ((((( U8 then hot floor talented'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_unigram(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weighted_choice_ngram(cur_word,freq_dist) :\n",
    "    ''' Starts with a current word and randomly chooses \n",
    "        a following word based on the bigrams. '''\n",
    "    \n",
    "    # First, build list of tuples of the form\n",
    "    # ('a_word',count)\n",
    "    # where our freq_dist has an entry like \n",
    "    # ('cur_word','a_word',count)\n",
    "    sub_dist = {}\n",
    "    \n",
    "    for bigram, count in freq_dist.items() :\n",
    "        if bigram[0] == cur_word :\n",
    "            sub_dist[bigram[1]] = count\n",
    "    \n",
    "    return(weighted_choice(sub_dist))\n",
    "\n",
    "def generate_bigram(text,length=10,start=None) :\n",
    "    \n",
    "    if not start :\n",
    "        uni_fd = FreqDist(text)\n",
    "        start = weighted_choice(uni_fd)\n",
    "        \n",
    "    fd = FreqDist(nltk.bigrams(text))\n",
    "    \n",
    "    results = [start]\n",
    "    this_word = start\n",
    "    for i in range(length) :\n",
    "        this_word = weighted_choice_ngram(this_word,fd)\n",
    "        results.append(this_word)\n",
    "        \n",
    "    return(\" \".join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'announced : that fish . We all he could only'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_bigram(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have been written to us , there for bringing matters'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_bigram(text2,start=\"I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conway PART JOIN PART damn it from texas area i'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_bigram(text5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
