{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Questions:\n",
    "\n",
    "1. Find emojis in the chat corpus.\n",
    "\n",
    "1. Determine a normalization scheme. (What needs to be normalized, how would you do it?)\n",
    "\n",
    "1. Count the happy vs sad emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chat = text5 # give it a nice name. \n",
    "\n",
    "# Let's find emojis in chat. \n",
    "potential_emojis = {w for w in chat if \":\" in w or \";\" in w or \"=\" in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!=',\n",
       " '.:',\n",
       " '.;)',\n",
       " '//www.wunderground.com/cgi-bin/findweather/getForecast?query=95953#FIR',\n",
       " '10:49',\n",
       " '2:55',\n",
       " '3:45',\n",
       " '4:03',\n",
       " '6:38',\n",
       " '6:41',\n",
       " '6:51',\n",
       " '6:53',\n",
       " '7:45',\n",
       " '9:10',\n",
       " ':',\n",
       " ':(',\n",
       " ':)',\n",
       " ':):):)',\n",
       " ':-(',\n",
       " ':-)',\n",
       " ':-@',\n",
       " ':-o',\n",
       " ':.',\n",
       " ':/',\n",
       " ':@',\n",
       " ':D',\n",
       " ':O',\n",
       " ':P',\n",
       " ':]',\n",
       " ':beer:',\n",
       " ':blush:',\n",
       " ':love:',\n",
       " ':o *',\n",
       " ':p',\n",
       " ':tongue:',\n",
       " ':|',\n",
       " ';',\n",
       " '; ..',\n",
       " ';)',\n",
       " ';-(',\n",
       " ';-)',\n",
       " ';0',\n",
       " ';]',\n",
       " ';p',\n",
       " '=',\n",
       " \"='s\",\n",
       " '=(',\n",
       " '=)',\n",
       " '=-\\\\',\n",
       " '=/',\n",
       " '=D',\n",
       " '=O',\n",
       " '=[',\n",
       " '=]',\n",
       " '=p',\n",
       " '>:->',\n",
       " ']:)',\n",
       " 'capab;e',\n",
       " 'd=',\n",
       " 'http://forums.talkcity.com/tc-adults/start ',\n",
       " 'http://www.shadowbots.com',\n",
       " 'n;t',\n",
       " 'o<|=D'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potential_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we're catching some non-emojis, but let's assume we're getting most of the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':(',\n",
       " ':)',\n",
       " ':-(',\n",
       " ':-)',\n",
       " ':-@',\n",
       " ':-o',\n",
       " ':.',\n",
       " ':/',\n",
       " ':@',\n",
       " ':D',\n",
       " ':O',\n",
       " ':P',\n",
       " ':]',\n",
       " ':p',\n",
       " ':|',\n",
       " ';)',\n",
       " ';-(',\n",
       " ';-)',\n",
       " ';0',\n",
       " ';]',\n",
       " ';p',\n",
       " '=(',\n",
       " '=)',\n",
       " '=-\\\\',\n",
       " '=/',\n",
       " '=D',\n",
       " '=O',\n",
       " '=[',\n",
       " '=]',\n",
       " '=p']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are all oriented left-to-right, so let's make a regex to find them. \n",
    "emoji = re.compile(r\"^[:;=]-?[)(\\]PD@op|O]$\") # misses '>:->' and ']:)' and repeats. Insert shruggie\n",
    "emoji2 = re.compile(r\"^[:;=]-?.$\")\n",
    "emojis = {w for w in chat if emoji2.search(w)}\n",
    "sorted(emojis)\n",
    "#len(emojis)\n",
    "# could normalize by removing hyphens, case letters to upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# Count happy vs sad\n",
    "happy = [w for w in chat if w in {\":-)\",\":)\",\":D\",\";-)\",\"=)\"}]\n",
    "sad = [w for w in chat if w in {\":-(\",\":(\",\";-(\",\"=(\"}]\n",
    "\n",
    "print(len(happy))\n",
    "print(len(sad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stemming\n",
    "\n",
    "Let's go through some stemming examples from the NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vowels = re.compile(r'[aeiouyAEIOU]')\n",
    "\n",
    "len({w for w in nltk.corpus.words.words() if not vowels.search(w[:-3]) and w[-3:] == \"ing\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aid of that Almighty Power which has hitherto protected me and enabled me to bring to favorable issues other important but still greatly inferior trusts heretofore confided to me by my country . The broad foundation upon which our Constitution rests being the people -- a breath of theirs having made , as a breath can unmake , change , or modify it -- it can be assigned to none of the great divisions of government but to that of democracy . If such is its theory , those who are called upon to administer it must recognize as its\n",
      "\n",
      "\n",
      "\n",
      "aid of that almighti power which ha hitherto protect me and enabl me to bring to favor issu other import but still greatli inferior trust heretofor confid to me by my countri . the broad foundat upon which our constitut rest be the peopl -- a breath of their have made , as a breath can unmak , chang , or modifi it -- it can be assign to none of the great divis of govern but to that of democraci . If such is it theori , those who are call upon to administ it must recogn as it\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer() # give it a short name.\n",
    "start = 30000\n",
    "distance = 100\n",
    "\n",
    "print(\" \".join(text4[start:(start + distance)]))\n",
    "print(\"\\n\\n\")\n",
    "print(\" \".join([porter.stem(w) for w in text4[start:(start + distance)]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9754\n"
     ]
    }
   ],
   "source": [
    "# words in inaugural addresses\n",
    "print(len(set(text4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5470\n",
      "1.783180987202925\n"
     ]
    }
   ],
   "source": [
    "inaug_stemmed = {porter.stem(w.lower()) for w in text4}\n",
    "\n",
    "print(len(inaug_stemmed))\n",
    "\n",
    "print(len(set(text4))/len(inaug_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Language Models\n",
    "Let's find some common n-grams in S&S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = FreqDist(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01443041193422614"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.freq('a') # what does this mean? It's the number of times \"a\" is used in \"Sense & Sensibility\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " 'couldn',\n",
       " 'didn',\n",
       " 'doesn',\n",
       " 'hadn',\n",
       " 'hasn',\n",
       " 'haven',\n",
       " 'isn',\n",
       " 'ma',\n",
       " 'mightn',\n",
       " 'mustn',\n",
       " 'needn',\n",
       " 'shan',\n",
       " 'shouldn',\n",
       " 'wasn',\n",
       " 'weren',\n",
       " 'won',\n",
       " 'wouldn']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elinor : 685 : 0.012688474789760307\n",
      "could : 578 : 0.010706479457637166\n",
      "marianne : 566 : 0.010484199607305598\n",
      "mrs : 530 : 0.009817360056310896\n",
      "would : 515 : 0.009539510243396436\n",
      "said : 397 : 0.0073537583818026895\n",
      "every : 377 : 0.0069832919645834105\n",
      "one : 331 : 0.006131219204979069\n",
      "much : 290 : 0.005371763049679547\n",
      "must : 283 : 0.005242099803652799\n",
      "sister : 282 : 0.005223576482791835\n",
      "edward : 263 : 0.00487163338643352\n",
      "mother : 258 : 0.0047790167821287\n",
      "dashwood : 252 : 0.004667876856962916\n",
      "well : 240 : 0.004445597006631349\n",
      "time : 239 : 0.004427073685770385\n",
      "know : 232 : 0.004297410439743637\n",
      "jennings : 230 : 0.004260363798021709\n",
      "though : 216 : 0.004001037305968214\n",
      "willoughby : 216 : 0.004001037305968214\n"
     ]
    }
   ],
   "source": [
    "fd = FreqDist([w.lower() for w in text2 \n",
    "               if w.lower() not in \n",
    "               nltk.corpus.stopwords.words(\"english\") \n",
    "               and w.isalpha()])\n",
    "\n",
    "total_words = sum([count for word, count in fd.items()])\n",
    "\n",
    "for pairs in fd.most_common(20) :\n",
    "    print(\" : \".join([pairs[0],str(pairs[1]),str(pairs[1]/total_words)]))\n",
    "    \n",
    "#4063/3861"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = FreqDist([\" \".join(b) for b in nltk.ngrams(text2,3) if b[0] == \"I\" and b[1] == \"am\"]) # could use bigram function instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I am sure', 72),\n",
       " ('I am not', 12),\n",
       " ('I am afraid', 11),\n",
       " ('I am so', 11),\n",
       " ('I am sorry', 11),\n",
       " ('I am very', 10),\n",
       " ('I am glad', 4),\n",
       " ('I am now', 4),\n",
       " ('I am monstrous', 4),\n",
       " ('I am convinced', 3)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am sure: 72\n",
      "I am convinced: 3\n",
      "I am afraid: 11\n",
      "I am by: 1\n",
      "I am very: 10\n",
      "I am before: 1\n",
      "I am glad: 4\n",
      "I am able: 1\n",
      "I am ready: 1\n",
      "I am not: 12\n",
      "I am much: 3\n",
      "I am with: 1\n",
      "I am almost: 2\n",
      "I am particularly: 2\n",
      "I am the: 2\n",
      "I am heartily: 1\n",
      "I am ,\": 1\n",
      "I am now: 4\n",
      "I am unable: 1\n",
      "I am persuaded: 1\n",
      "I am happy: 1\n",
      "I am perfectly: 3\n",
      "I am guilty: 1\n",
      "I am so: 11\n",
      "I am only: 2\n",
      "I am going: 1\n",
      "I am ,: 3\n",
      "I am confined: 1\n",
      "I am monstrous: 4\n",
      "I am flattered: 1\n",
      "I am .: 1\n",
      "I am always: 3\n",
      "I am at: 1\n",
      "I am ;: 1\n",
      "I am sorry: 11\n",
      "I am a: 1\n",
      "I am determined: 1\n",
      "I am in: 3\n",
      "I am alive: 2\n",
      "I am resolved: 1\n",
      "I am rather: 1\n",
      "I am bound: 1\n",
      "I am capable: 2\n",
      "I am delighted: 1\n",
      "I am ever: 1\n",
      "I am writing: 1\n",
      "I am NOT: 1\n",
      "I am commissioned: 1\n",
      "I am quite: 2\n",
      "I am miserable: 1\n",
      "I am to: 2\n",
      "I am wretched: 1\n",
      "I am justified: 1\n",
      "I am right: 1\n",
      "I am well: 2\n",
      "I am amazingly: 1\n",
      "I am come: 1\n",
      "I am extremely: 2\n",
      "I am excessively: 1\n",
      "I am informed: 1\n",
      "I am charged: 1\n",
      "I am no: 1\n",
      "I am talking: 1\n",
      "I am ruined: 1\n",
      "I am shut: 1\n",
      "I am allowed: 1\n",
      "I am thankful: 1\n",
      "I am doing: 1\n",
      "I am grown: 2\n",
      "I am disappointed: 1\n"
     ]
    }
   ],
   "source": [
    "for gram, count in fd.items() :\n",
    "    if gram[0] == \"I\" :\n",
    "        print(gram + \": \" + str(count))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_words = sum([count for pair, count in fd.items() if pair[0] == \"I\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am sure\n",
      "72\n",
      "I am not\n",
      "12\n",
      "I am afraid\n",
      "11\n",
      "I am so\n",
      "11\n",
      "I am sorry\n",
      "11\n",
      "I am very\n",
      "10\n",
      "I am glad\n",
      "4\n",
      "I am now\n",
      "4\n",
      "I am monstrous\n",
      "4\n",
      "I am convinced\n",
      "3\n",
      "I am much\n",
      "3\n",
      "I am perfectly\n",
      "3\n",
      "I am ,\n",
      "3\n",
      "I am always\n",
      "3\n",
      "I am in\n",
      "3\n",
      "I am almost\n",
      "2\n",
      "I am particularly\n",
      "2\n",
      "I am the\n",
      "2\n",
      "I am only\n",
      "2\n",
      "I am alive\n",
      "2\n",
      "I am capable\n",
      "2\n",
      "I am quite\n",
      "2\n",
      "I am to\n",
      "2\n",
      "I am well\n",
      "2\n",
      "I am extremely\n",
      "2\n",
      "I am grown\n",
      "2\n",
      "I am by\n",
      "1\n",
      "I am before\n",
      "1\n",
      "I am able\n",
      "1\n",
      "I am ready\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for gram, count in fd.most_common(30) :\n",
    "    if gram[0] == \"I\" : \n",
    "        print(gram)\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am sure : 72 : 0.323\n",
      "I am not : 12 : 0.054\n",
      "I am afraid : 11 : 0.049\n",
      "I am so : 11 : 0.049\n",
      "I am sorry : 11 : 0.049\n",
      "I am very : 10 : 0.045\n",
      "I am glad : 4 : 0.018\n",
      "I am now : 4 : 0.018\n",
      "I am monstrous : 4 : 0.018\n",
      "I am convinced : 3 : 0.013\n",
      "I am much : 3 : 0.013\n",
      "I am perfectly : 3 : 0.013\n",
      "I am , : 3 : 0.013\n",
      "I am always : 3 : 0.013\n",
      "I am in : 3 : 0.013\n",
      "I am almost : 2 : 0.009\n",
      "I am particularly : 2 : 0.009\n",
      "I am the : 2 : 0.009\n",
      "I am only : 2 : 0.009\n",
      "I am alive : 2 : 0.009\n",
      "I am capable : 2 : 0.009\n",
      "I am quite : 2 : 0.009\n",
      "I am to : 2 : 0.009\n",
      "I am well : 2 : 0.009\n",
      "I am extremely : 2 : 0.009\n",
      "I am grown : 2 : 0.009\n",
      "I am by : 1 : 0.004\n",
      "I am before : 1 : 0.004\n",
      "I am able : 1 : 0.004\n",
      "I am ready : 1 : 0.004\n",
      "I am with : 1 : 0.004\n",
      "I am heartily : 1 : 0.004\n",
      "I am ,\" : 1 : 0.004\n",
      "I am unable : 1 : 0.004\n",
      "I am persuaded : 1 : 0.004\n",
      "I am happy : 1 : 0.004\n",
      "I am guilty : 1 : 0.004\n",
      "I am going : 1 : 0.004\n",
      "I am confined : 1 : 0.004\n",
      "I am flattered : 1 : 0.004\n",
      "I am . : 1 : 0.004\n",
      "I am at : 1 : 0.004\n",
      "I am ; : 1 : 0.004\n",
      "I am a : 1 : 0.004\n",
      "I am determined : 1 : 0.004\n",
      "I am resolved : 1 : 0.004\n",
      "I am rather : 1 : 0.004\n",
      "I am bound : 1 : 0.004\n",
      "I am delighted : 1 : 0.004\n",
      "I am ever : 1 : 0.004\n",
      "I am writing : 1 : 0.004\n",
      "I am NOT : 1 : 0.004\n",
      "I am commissioned : 1 : 0.004\n",
      "I am miserable : 1 : 0.004\n",
      "I am wretched : 1 : 0.004\n",
      "I am justified : 1 : 0.004\n",
      "I am right : 1 : 0.004\n",
      "I am amazingly : 1 : 0.004\n",
      "I am come : 1 : 0.004\n",
      "I am excessively : 1 : 0.004\n",
      "I am informed : 1 : 0.004\n",
      "I am charged : 1 : 0.004\n",
      "I am no : 1 : 0.004\n",
      "I am talking : 1 : 0.004\n",
      "I am ruined : 1 : 0.004\n",
      "I am shut : 1 : 0.004\n",
      "I am allowed : 1 : 0.004\n",
      "I am thankful : 1 : 0.004\n",
      "I am doing : 1 : 0.004\n",
      "I am disappointed : 1 : 0.004\n"
     ]
    }
   ],
   "source": [
    "for gram,count in sorted(fd.items(), key=lambda pair: pair[1], reverse=True) : \n",
    "    if gram[0] == \"I\" :\n",
    "        print(\" : \".join([str(gram),str(count),str(round(count/total_words,3))]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I am sure', 72),\n",
       " ('I am not', 12),\n",
       " ('I am afraid', 11),\n",
       " ('I am so', 11),\n",
       " ('I am sorry', 11),\n",
       " ('I am very', 10),\n",
       " ('I am glad', 4),\n",
       " ('I am now', 4),\n",
       " ('I am monstrous', 4),\n",
       " ('I am convinced', 3)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd = FreqDist(nltk.ngrams(text2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'am', 'sure') : 72\n",
      "('I', 'am', 'not') : 12\n",
      "('I', 'am', 'afraid') : 11\n",
      "('I', 'am', 'so') : 11\n",
      "('I', 'am', 'sorry') : 11\n",
      "('I', 'am', 'very') : 10\n",
      "('I', 'am', 'glad') : 4\n",
      "('I', 'am', 'now') : 4\n",
      "('I', 'am', 'monstrous') : 4\n",
      "('I', 'am', 'convinced') : 3\n",
      "('I', 'am', 'much') : 3\n",
      "('I', 'am', 'perfectly') : 3\n",
      "('I', 'am', ',') : 3\n",
      "('I', 'am', 'always') : 3\n",
      "('I', 'am', 'in') : 3\n",
      "('I', 'am', 'almost') : 2\n",
      "('I', 'am', 'particularly') : 2\n",
      "('I', 'am', 'the') : 2\n",
      "('I', 'am', 'only') : 2\n",
      "('I', 'am', 'alive') : 2\n",
      "('I', 'am', 'capable') : 2\n",
      "('I', 'am', 'quite') : 2\n",
      "('I', 'am', 'to') : 2\n",
      "('I', 'am', 'well') : 2\n",
      "('I', 'am', 'extremely') : 2\n",
      "('I', 'am', 'grown') : 2\n",
      "('I', 'am', 'by') : 1\n",
      "('I', 'am', 'before') : 1\n",
      "('I', 'am', 'able') : 1\n",
      "('I', 'am', 'ready') : 1\n",
      "('I', 'am', 'with') : 1\n",
      "('I', 'am', 'heartily') : 1\n",
      "('I', 'am', ',\"') : 1\n",
      "('I', 'am', 'unable') : 1\n",
      "('I', 'am', 'persuaded') : 1\n",
      "('I', 'am', 'happy') : 1\n",
      "('I', 'am', 'guilty') : 1\n",
      "('I', 'am', 'going') : 1\n",
      "('I', 'am', 'confined') : 1\n",
      "('I', 'am', 'flattered') : 1\n",
      "('I', 'am', '.') : 1\n",
      "('I', 'am', 'at') : 1\n",
      "('I', 'am', ';') : 1\n",
      "('I', 'am', 'a') : 1\n",
      "('I', 'am', 'determined') : 1\n",
      "('I', 'am', 'resolved') : 1\n",
      "('I', 'am', 'rather') : 1\n",
      "('I', 'am', 'bound') : 1\n",
      "('I', 'am', 'delighted') : 1\n",
      "('I', 'am', 'ever') : 1\n",
      "('I', 'am', 'writing') : 1\n",
      "('I', 'am', 'NOT') : 1\n",
      "('I', 'am', 'commissioned') : 1\n",
      "('I', 'am', 'miserable') : 1\n",
      "('I', 'am', 'wretched') : 1\n",
      "('I', 'am', 'justified') : 1\n",
      "('I', 'am', 'right') : 1\n",
      "('I', 'am', 'amazingly') : 1\n",
      "('I', 'am', 'come') : 1\n",
      "('I', 'am', 'excessively') : 1\n",
      "('I', 'am', 'informed') : 1\n",
      "('I', 'am', 'charged') : 1\n",
      "('I', 'am', 'no') : 1\n",
      "('I', 'am', 'talking') : 1\n",
      "('I', 'am', 'ruined') : 1\n",
      "('I', 'am', 'shut') : 1\n",
      "('I', 'am', 'allowed') : 1\n",
      "('I', 'am', 'thankful') : 1\n",
      "('I', 'am', 'doing') : 1\n",
      "('I', 'am', 'disappointed') : 1\n",
      "0.32286995515695066\n",
      "0.053811659192825115\n"
     ]
    }
   ],
   "source": [
    "total_words = 0\n",
    "\n",
    "for gram,count in sorted(fd.items(), key=lambda pair: pair[1], reverse=True) : \n",
    "    if gram[0] == \"I\" and gram[1] == \"am\" :\n",
    "        total_words += count\n",
    "        print(\" : \".join([str(gram),str(count)])) \n",
    "        \n",
    "\n",
    "print(72/total_words)\n",
    "print(12/total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 2004 matches:\n",
      " to me ,\" replied her husband , \" that I should assist his widow and daughters \n",
      " did not know what he was talking of , I dare say ; ten to one but he was light\n",
      "ly to myself . He could hardly suppose I should neglect them . But as he requir\n",
      "hem . But as he required the promise , I could not do less than give it ; at le\n",
      "ld not do less than give it ; at least I thought so at the time . The promise ,\n",
      "t you have such a generous spirit !\" \" I would not wish to do any thing mean ,\"\n",
      "little . No one , at least , can think I have not done enough for them : even t\n",
      "can afford to do .\" \" Certainly -- and I think I may afford to give them five h\n",
      "rd to do .\" \" Certainly -- and I think I may afford to give them five hundred p\n",
      " That is very true , and , therefore , I do not know whether , upon the whole ,\n",
      " them -- something of the annuity kind I mean .-- My sisters would feel the goo\n",
      " are not aware of what you are doing . I have known a great deal of the trouble\n",
      "such an abhorrence of annuities , that I am sure I would not pin myself down to\n",
      "horrence of annuities , that I am sure I would not pin myself down to the payme\n",
      "and it raises no gratitude at all . If I were you , whatever I did should be do\n",
      "tude at all . If I were you , whatever I did should be done at my own discretio\n",
      "e done at my own discretion entirely . I would not bind myself to allow them an\n",
      "ifty pounds from our own expenses .\" \" I believe you are right , my love ; it w\n",
      "d by no annuity in the case ; whatever I may give them occasionally will be of \n",
      "eing distressed for money , and will , I think , be amply discharging my promis\n",
      " it will . Indeed , to say the truth , I am convinced within myself that your f\n",
      "t all . The assistance he thought of , I dare say , was only such as might be r\n",
      " forth , whenever they are in season . I ' ll lay my life that he meant nothing\n",
      "e they will be ! Five hundred a year ! I am sure I cannot imagine how they will\n",
      "l be ! Five hundred a year ! I am sure I cannot imagine how they will spend hal\n"
     ]
    }
   ],
   "source": [
    "text2.concordance(\"I\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## N-gram models\n",
    "\n",
    "Let's make a function that takes in text, builds a freq dist and generates text with various n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def weighted_choice(freq_dist):\n",
    "    weight_total = sum([count for token,count in freq_dist.items()])\n",
    "    n = random.uniform(0, weight_total)\n",
    "    for token, count in freq_dist.items() :\n",
    "        if n < count:\n",
    "            return(token)\n",
    "        n = n - count\n",
    "    return(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deal'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_choice(FreqDist(text5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_unigram(text,length=10) :\n",
    "    fd = FreqDist(text)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(length) :\n",
    "        results.append(weighted_choice(fd))\n",
    "        \n",
    "    return(\" \".join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In in what God come s and art is headmost'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_unigram(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ever Elinor her their attachment , a ; name him'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_unigram(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I right spell thing back . job strait here brightened'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_unigram(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weighted_choice_ngram(cur_word,freq_dist) :\n",
    "    ''' Starts with a current word and randomly chooses \n",
    "        a following word based on the bigrams. '''\n",
    "    \n",
    "    # First, build list of tuples of the form\n",
    "    # ('a_word',count)\n",
    "    # where our freq_dist has an entry like \n",
    "    # ('cur_word','a_word',count)\n",
    "    sub_dist = {}\n",
    "    \n",
    "    for bigram, count in freq_dist.items() :\n",
    "        if bigram[0] == cur_word :\n",
    "            sub_dist[bigram[1]] = count\n",
    "    \n",
    "    return(weighted_choice(sub_dist))\n",
    "\n",
    "def generate_bigram(text,length=10,start=None) :\n",
    "    \n",
    "    if not start :\n",
    "        uni_fd = FreqDist(text)\n",
    "        start = weighted_choice(uni_fd)\n",
    "        \n",
    "    fd = FreqDist(nltk.bigrams(text))\n",
    "    \n",
    "    results = []\n",
    "    this_word = start\n",
    "    for i in range(length) :\n",
    "        this_word = weighted_choice_ngram(this_word,fd)\n",
    "        results.append(this_word)\n",
    "        \n",
    "    return(\" \".join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. \" Turn to show his blood - mill .'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_bigram(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done wondering at present , \" Yes , and the'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_bigram(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes wasup room JOIN . Temp. -6 F / New'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_bigram(text5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
