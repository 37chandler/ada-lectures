{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Questions:\n",
    "\n",
    "1. Find emojis in the chat corpus.\n",
    "\n",
    "1. Determine a normalization scheme. (What needs to be normalized, how would you do it?)\n",
    "\n",
    "1. Count the happy vs sad emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chat = text5 # give it a nice name. \n",
    "\n",
    "# Let's find emojis in chat. \n",
    "potential_emojis = {w for w in chat if \":\" in w or \";\" in w or \"=\" in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!=',\n",
       " '.:',\n",
       " '.;)',\n",
       " '//www.wunderground.com/cgi-bin/findweather/getForecast?query=95953#FIR',\n",
       " '10:49',\n",
       " '2:55',\n",
       " '3:45',\n",
       " '4:03',\n",
       " '6:38',\n",
       " '6:41',\n",
       " '6:51',\n",
       " '6:53',\n",
       " '7:45',\n",
       " '9:10',\n",
       " ':',\n",
       " ':(',\n",
       " ':)',\n",
       " ':):):)',\n",
       " ':-(',\n",
       " ':-)',\n",
       " ':-@',\n",
       " ':-o',\n",
       " ':.',\n",
       " ':/',\n",
       " ':@',\n",
       " ':D',\n",
       " ':O',\n",
       " ':P',\n",
       " ':]',\n",
       " ':beer:',\n",
       " ':blush:',\n",
       " ':love:',\n",
       " ':o *',\n",
       " ':p',\n",
       " ':tongue:',\n",
       " ':|',\n",
       " ';',\n",
       " '; ..',\n",
       " ';)',\n",
       " ';-(',\n",
       " ';-)',\n",
       " ';0',\n",
       " ';]',\n",
       " ';p',\n",
       " '=',\n",
       " \"='s\",\n",
       " '=(',\n",
       " '=)',\n",
       " '=-\\\\',\n",
       " '=/',\n",
       " '=D',\n",
       " '=O',\n",
       " '=[',\n",
       " '=]',\n",
       " '=p',\n",
       " '>:->',\n",
       " ']:)',\n",
       " 'capab;e',\n",
       " 'd=',\n",
       " 'http://forums.talkcity.com/tc-adults/start ',\n",
       " 'http://www.shadowbots.com',\n",
       " 'n;t',\n",
       " 'o<|=D'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potential_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we're catching some non-emojis, but let's assume we're getting most of the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':(',\n",
       " ':)',\n",
       " ':-(',\n",
       " ':-)',\n",
       " ':-@',\n",
       " ':-o',\n",
       " ':@',\n",
       " ':D',\n",
       " ':O',\n",
       " ':P',\n",
       " ':]',\n",
       " ':p',\n",
       " ':|',\n",
       " ';)',\n",
       " ';-(',\n",
       " ';-)',\n",
       " ';]',\n",
       " ';p',\n",
       " '=(',\n",
       " '=)',\n",
       " '=D',\n",
       " '=O',\n",
       " '=]',\n",
       " '=p']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We haven't talked regex yet, but here's a regex that finds most\n",
    "# left-to-right emojis\n",
    "emoji = re.compile(r\"^[:;=]-?[)(\\]PD@op|O]$\") # misses '>:->' and ']:)' and repeats. Insert shruggie\n",
    "emojis = {w for w in chat if emoji.search(w)}\n",
    "sorted(emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Count happy vs sad\n",
    "happy_emojis = [\":-)\"] # add others here. \n",
    "sad_emojis = []\n",
    "\n",
    "happy = [w for w in chat if w in happy_emojis]\n",
    "sad = [w for w in chat if w in sad_emojis]\n",
    "\n",
    "print(len(happy))\n",
    "print(len(sad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stemming\n",
    "\n",
    "Let's go through some stemming examples from the NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count how many words in nltk.corpus.words.words() have the following two-letter combinations\n",
    "\n",
    "#pattern = \"ng\"\n",
    "\n",
    "#pattern = \"st\"\n",
    "\n",
    "pattern = \"tk\"\n",
    "\n",
    "len([w for w in nltk.corpus.words.words() if pattern in w])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aitkenite',\n",
       " 'Atka',\n",
       " 'boatkeeper',\n",
       " 'catkin',\n",
       " 'catkinate',\n",
       " 'doitkin',\n",
       " 'dotkin',\n",
       " 'giantkind',\n",
       " 'Gitksan',\n",
       " 'hutkeeper',\n",
       " 'Jatki',\n",
       " 'jutka',\n",
       " 'Kamchatkan',\n",
       " 'kibitka',\n",
       " 'Kitkahaxki',\n",
       " 'Kitkehahki',\n",
       " 'lightkeeper',\n",
       " 'matka',\n",
       " 'Nootka',\n",
       " 'Notkerian',\n",
       " 'otkon',\n",
       " 'outkeeper',\n",
       " 'outkick',\n",
       " 'outkill',\n",
       " 'outking',\n",
       " 'outkiss',\n",
       " 'outkitchen',\n",
       " 'outknave',\n",
       " 'outknee',\n",
       " 'petkin',\n",
       " 'planetkin',\n",
       " 'pocketknife',\n",
       " 'Sitka',\n",
       " 'Sitkan',\n",
       " 'thoughtkin',\n",
       " 'vetkousie',\n",
       " 'whatkin',\n",
       " 'wicketkeep',\n",
       " 'wicketkeeper',\n",
       " 'wicketkeeping']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in nltk.corpus.words.words() if pattern in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bonus, what two-letter combinations are the least common in English? What are the most? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aid of that Almighty Power which has hitherto protected me and enabled me to bring to favorable issues other important but still greatly inferior trusts heretofore confided to me by my country . The broad foundation upon which our Constitution rests being the people -- a breath of theirs having made , as a breath can unmake , change , or modify it -- it can be assigned to none of the great divisions of government but to that of democracy . If such is its theory , those who are called upon to administer it must recognize as its\n",
      "\n",
      "\n",
      "\n",
      "aid of that almighti power which ha hitherto protect me and enabl me to bring to favor issu other import but still greatli inferior trust heretofor confid to me by my countri . the broad foundat upon which our constitut rest be the peopl -- a breath of their have made , as a breath can unmak , chang , or modifi it -- it can be assign to none of the great divis of govern but to that of democraci . If such is it theori , those who are call upon to administ it must recogn as it\n"
     ]
    }
   ],
   "source": [
    "# Let's compare stemmed text to not-stemmed. \n",
    "\n",
    "porter = nltk.PorterStemmer() # give it a short name.\n",
    "start = 30000\n",
    "distance = 100\n",
    "\n",
    "print(\" \".join(text4[start:(start + distance)]))\n",
    "print(\"\\n\\n\")\n",
    "print(\" \".join([porter.stem(w) for w in text4[start:(start + distance)]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count the number of unique words in the inaugural address \n",
    "# corpus.\n",
    "\n",
    "# Now cast to lower-case and stem. Count those unique tokens\n",
    "\n",
    "# what does the ratio tell us?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Language Models\n",
    "Let's find some common n-grams in S&S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = FreqDist(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd.freq('a') # what does this mean? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's look at the stopwords in English\n",
    "nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this next set of code. What does it do?\n",
    "\n",
    "fd = FreqDist([w.lower() for w in text2 \n",
    "               if w.lower() not in \n",
    "               nltk.corpus.stopwords.words(\"english\") \n",
    "               and w.isalpha()])\n",
    "\n",
    "total_words = sum([count for word, count in fd.items()])\n",
    "\n",
    "for pairs in fd.most_common(20) :\n",
    "    print(\" : \".join([pairs[0],str(pairs[1]),str(pairs[1]/total_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = FreqDist([\" \".join(b) for b in nltk.ngrams(text2,3) if b[0] == \"I\" and b[1] == \"am\"]) # could use bigram function instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_words = sum([count for pair, count in fd.items() if pair[0] == \"I\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for gram,count in sorted(fd.items(), key=lambda pair: pair[1], reverse=True) : \n",
    "    if gram[0] == \"I\" :\n",
    "        print(\" : \".join([str(gram),str(count),str(round(count/total_words,3))]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd = FreqDist(nltk.ngrams(text2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words = 0\n",
    "\n",
    "for gram,count in sorted(fd.items(), key=lambda pair: pair[1], reverse=True) : \n",
    "    if gram[0] == \"I\" and gram[1] == \"am\" :\n",
    "        total_words += count\n",
    "        print(\" : \".join([str(gram),str(count)])) \n",
    "        \n",
    "\n",
    "print(72/total_words)\n",
    "print(12/total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text2.concordance(\"I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text2.concordance(\"sure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## N-gram models\n",
    "\n",
    "Let's make a function that takes in text, builds a freq dist and generates text with various n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Write a function called weighted_choice that selects \n",
    "# a word from a FreqDist based on the probability\n",
    "# derived from the count of that word compared to the\n",
    "# count of all words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weighted_choice(FreqDist(text5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_unigram(text,length=10) :\n",
    "    # A function that generates `length` words from\n",
    "    # a body of text using `weighted_choice`\n",
    "    \n",
    "    fd = FreqDist(text)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(length) :\n",
    "        results.append(weighted_choice(fd))\n",
    "        \n",
    "    return(\" \".join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_unigram(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_unigram(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_unigram(text5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the above, but have it work correctly with bi-grams. This is a bit trickier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_bigram(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_bigram(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_bigram(text5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
