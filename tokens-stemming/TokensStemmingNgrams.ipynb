{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Questions:\n",
    "\n",
    "1. Find emojis in the chat corpus.\n",
    "\n",
    "1. Determine a normalization scheme. (What needs to be normalized, how would you do it?)\n",
    "\n",
    "1. Count the happy vs sad emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chat = text5 # give it a nice name. \n",
    "\n",
    "# Let's find emojis in chat. \n",
    "potential_emojis = {w for w in chat if \":\" in w or \";\" in w or \"=\" in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "potential_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we're catching some non-emojis, but let's assume we're getting most of the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We haven't talked regex yet, but here's a regex that finds most\n",
    "# left-to-right emojis\n",
    "emoji = re.compile(r\"^[:;=]-?[)(\\]PD@op|O]$\") # misses '>:->' and ']:)' and repeats. Insert shruggie\n",
    "emojis = {w for w in chat if emoji.search(w)}\n",
    "sorted(emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count happy vs sad\n",
    "happy_emojis = [\":-)\"] # add others here. \n",
    "sad_emojis = []\n",
    "\n",
    "happy = [w for w in chat if w in happy_emojis]\n",
    "sad = [w for w in chat if w in sad_emojis]\n",
    "\n",
    "print(len(happy))\n",
    "print(len(sad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stemming\n",
    "\n",
    "Let's go through some stemming examples from the NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count how many words in nltk.corpus.words.words() have the following two-letter combinations\n",
    "\n",
    "pattern = \"ng\"\n",
    "\n",
    "pattern = \"st\"\n",
    "\n",
    "pattern = \"tk\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bonus, what two-letter combinations are the least common in English? What are the most? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's compare stemmed text to not-stemmed. \n",
    "\n",
    "porter = nltk.PorterStemmer() # give it a short name.\n",
    "start = 30000\n",
    "distance = 100\n",
    "\n",
    "print(\" \".join(text4[start:(start + distance)]))\n",
    "print(\"\\n\\n\")\n",
    "print(\" \".join([porter.stem(w) for w in text4[start:(start + distance)]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count the number of unique words in the inaugural address \n",
    "# corpus.\n",
    "\n",
    "# Now cast to lower-case and stem. Count those unique tokens\n",
    "\n",
    "# what does the ratio tell us?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Language Models\n",
    "Let's find some common n-grams in S&S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = FreqDist(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd.freq('a') # what does this mean? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's look at the stopwords in English\n",
    "nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this next set of code. What does it do?\n",
    "\n",
    "fd = FreqDist([w.lower() for w in text2 \n",
    "               if w.lower() not in \n",
    "               nltk.corpus.stopwords.words(\"english\") \n",
    "               and w.isalpha()])\n",
    "\n",
    "total_words = sum([count for word, count in fd.items()])\n",
    "\n",
    "for pairs in fd.most_common(20) :\n",
    "    print(\" : \".join([pairs[0],str(pairs[1]),str(pairs[1]/total_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = FreqDist([\" \".join(b) for b in nltk.ngrams(text2,3) if b[0] == \"I\" and b[1] == \"am\"]) # could use bigram function instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_words = sum([count for pair, count in fd.items() if pair[0] == \"I\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for gram,count in sorted(fd.items(), key=lambda pair: pair[1], reverse=True) : \n",
    "    if gram[0] == \"I\" :\n",
    "        print(\" : \".join([str(gram),str(count),str(round(count/total_words,3))]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd = FreqDist(nltk.ngrams(text2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words = 0\n",
    "\n",
    "for gram,count in sorted(fd.items(), key=lambda pair: pair[1], reverse=True) : \n",
    "    if gram[0] == \"I\" and gram[1] == \"am\" :\n",
    "        total_words += count\n",
    "        print(\" : \".join([str(gram),str(count)])) \n",
    "        \n",
    "\n",
    "print(72/total_words)\n",
    "print(12/total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text2.concordance(\"I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text2.concordance(\"sure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## N-gram models\n",
    "\n",
    "Let's make a function that takes in text, builds a freq dist and generates text with various n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Write a function called weighted_choice that selects \n",
    "# a word from a FreqDist based on the probability\n",
    "# derived from the count of that word compared to the\n",
    "# count of all words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weighted_choice(FreqDist(text5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_unigram(text,length=10) :\n",
    "    # A function that generates `length` words from\n",
    "    # a body of text using `weighted_choice`\n",
    "    \n",
    "    fd = FreqDist(text)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(length) :\n",
    "        results.append(weighted_choice(fd))\n",
    "        \n",
    "    return(\" \".join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_unigram(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_unigram(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_unigram(text5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the above, but have it work correctly with bi-grams. This is a bit trickier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_bigram(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_bigram(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_bigram(text5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
