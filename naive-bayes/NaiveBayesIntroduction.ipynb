{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifiers: Introduction\n",
    "A powerful and intutitive technique. File this one away, it'll often teach you a lot about a problem, even if it doesn't \"win\" the accuracy game. First some examples from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "\n",
    "# Create some labeled observations\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "                 [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "# shuffle so that we can have a training and test set\n",
    "random.shuffle(labeled_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at labeled_names, to get a sense for what's in there. This is always a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeled_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For the purposes of this toy example, we just use the last letters as our only feature\n",
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next line, read a bit about what's going on with this classifier [here](http://www.nltk.org/book/ch06.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at `featuresets`. What kind of data structure is it? What are the elements within it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NLTK makes it easy to evaluate the accuracy of the rule.\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the classifier does on our class. Fill in the gaps below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "our_class = # make this a list of first names in our class\n",
    "\n",
    "for student in our_class :\n",
    "    print(student + \" classified as \" + classifier.classify(gender_features(student)))\n",
    "\n",
    "# What's the overall accuracy? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might reasonably ask, how many males and females do we have in each group? Below we see two ways of displaying that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This method takes more typing, but may \n",
    "# be easier to read.\n",
    "\n",
    "num_males = 0\n",
    "\n",
    "for item in featuresets :\n",
    "    dd, gender = item\n",
    "        \n",
    "    if gender == \"male\" :\n",
    "        num_males += 1\n",
    "    \n",
    "num_males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This approach is more pythonic, but also harder to understand.\n",
    "# When you try to interpret it, remember to start with the innermost\n",
    "# part (probably the `for` loop here). \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "Counter([gender for dd, gender in featuresets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's just look at all the features. Usually you'd only show a few\n",
    "classifier.show_most_informative_features(26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we interpret those columns above? \n",
    "\n",
    "--- \n",
    "\n",
    "The lecture mentions the idea of building a dev-test set, in addition to the test and train sets above. Let's do that now so that we can build up some more complicated feature extractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(labeled_names) # Use this to shuffle in place to build training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_size = 500\n",
    "devtest_size = 1000\n",
    "\n",
    "train_names = labeled_names[(test_size + devtest_size):]\n",
    "devtest_names = labeled_names[test_size:(test_size + devtest_size)]\n",
    "test_names = labeled_names[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    guess = classifier.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append( (tag, guess, name) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below. Look at the kind of names that are being misclassified. As you do that, think about rules you migth design that would correct these mistakes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (tag, guess, name) in sorted(errors):\n",
    "    print('correct={:<8} guess={:<8s} name={:<30}'.format(tag, guess, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're going to start building your own feature extractor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build your own function. Here's an example to\n",
    "# help you get the syntax right. \n",
    "def gender_features_2(word):\n",
    "    ''' This function should take in a word and return a dictionary\n",
    "        with the name of the feature as the key and the value \n",
    "        as the feature value. '''\n",
    "    ll = word[-1]\n",
    "    penultimate = word[-2]\n",
    "    last_3 = word[-3:]\n",
    "    \n",
    "    has_bob = \"bob\" in word\n",
    "        \n",
    "    ret_dict = {'last_letter':ll,\n",
    "                'penultimate_y':(penultimate==\"y\"),\n",
    "                'last_3':last_3,\n",
    "                'has_bob' : has_bob}\n",
    "    \n",
    "    return (ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's look at an output\n",
    "gender_features_2(\"bobby\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's form our new training and dev-test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = [(gender_features_2(n), gender) for (n, gender) in train_names]\n",
    "devtest_set = [(gender_features_2(n), gender) for (n, gender) in devtest_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this new code on the training set and evaluate it on the _development_ test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the most informative features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And look at where we're getting errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    guess = classifier.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append( (tag, guess, name) )\n",
    "\n",
    "for (tag, guess, name) in sorted(errors):\n",
    "    print('correct={:<8} guess={:<8s} name={:<30}'.format(tag, guess, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll refine `gender_features_2`. Go through the errors above, try new rules. Can you come up with any that drammatically increase the accuracy of your classifer? You should be able to get this above 82% accuracy with some experimentation. What's the highest value you can get? \n",
    "\n",
    "--- \n",
    "\n",
    "Once you're done tweaking your code or we're out of time, get your final accuracy measure against the test set. In order to have an unbiased estimate of your error, you need to do this once at the end of your development cycle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Once you're done tweaking your code, run this one. \n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
