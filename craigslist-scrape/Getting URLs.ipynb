{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Craigslist Car Scraping\n",
    "\n",
    "One of the ongoing projects we'll work on is scraping CL car listings. This workbook provides a starting point for the project. There are two big things we need to do in order to scrap CL: figure out the new listings and extract the data from those pages. \n",
    "\n",
    "Note, CL's [Terms of Use](https://www.craigslist.org/about/terms.of.use?lang=en&cc=us) do not allow web scraping, though the specific actions we're taking aren't named in the scary `LIQUIDATED DAMAGES` section. Let's consider this a gray area, but let's be careful and request pages at a reasonable rate. If you bombard the site, your IP could get blacklisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "city = \"seattle\"         # this should be a valid CL city name\n",
    "search_term = \"tacoma\"   # pick your own!\n",
    "\n",
    "\n",
    "rss_url_start = \"https://\"\n",
    "rss_url_end = \".craigslist.org/search/cto\"\n",
    "params = {'format':'rss',\n",
    "          'query':search_term,\n",
    "          's':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's pull the RSS listings for this page. \n",
    "rss_page = requests.get(\"\".join([rss_url_start,city,rss_url_end]),\n",
    "                       params=params)\n",
    "rss_soup = BeautifulSoup(rss_page.text,\"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've pulled the RSS page, let's take a look at the HTML. What do we want out of this? Compare it to the source of the URL, which you can view by putting `rss_page.url` in a cell and executing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rss_soup.html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, it looks like the URls we need are in a tag with `rdf:li`. RDF stands for \"Resource Description Framework\", as I've discovered from 10 seconds of Googling. `li` stands for \"List Item\". We can iterate over these and extract the URLs. For now, we'll just store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls = []\n",
    "\n",
    "for item in rss_soup.find_all(b\"rdf:li\") :\n",
    "    pieces = str(item).split('\"')\n",
    "    urls.append(pieces[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the URLs. Did we get what we want? If so, go to a couple of the URLs and look at the page. Also write some Python code that pulls the page. Inspect the the HTML.\n",
    "\n",
    "What would we need to do to parse that page? \n",
    "\n",
    "--- \n",
    "\n",
    "Now write out your URLs to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your output code here.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
