{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell checking\n",
    "\n",
    "This workbook holds some exploration of the spell checking code.\n",
    "\n",
    "---\n",
    "\n",
    "Exercise 1: read in `wsj_with_errors.txt` into Python. Find every word in there that's not in the nltk word corpus and write that out to a new file. Remember to remove punctuation and numbers and to cast everything to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "input_file = \"wsj_with_errors.txt\"\n",
    "non_alpha = re.compile(r\"[^a-z ]\")\n",
    "\n",
    "# also consider the `isalpha` function\n",
    "print(\"abcde\".isalpha())\n",
    "print(\"ab123\".isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here's a place to see if you've stripped out everything.\n",
    "\n",
    "line = \"Welcome to ADA. This is week 15!\"\n",
    "\n",
    "# See if you can process this line and have it return a list of words\n",
    "# that are in lowercase, with no punctuation, and with no numbers.\n",
    "\n",
    "line = line.lower()\n",
    "line = \"\".join([ch for ch in line if not non_alpha.match(ch)])\n",
    "line.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_set = {w.lower() for w in words.words()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_missing_words = list()\n",
    "\n",
    "with open(input_file) as ifile :\n",
    "    for row in ifile :\n",
    "       \n",
    "        row = row.lower()\n",
    "        row = \"\".join([ch for ch in row if not non_alpha.match(ch)])\n",
    "        \n",
    "        row = row.strip().split()\n",
    "        \n",
    "        missing_words = [w for w in row if w not in word_set]\n",
    "\n",
    "        all_missing_words.extend(missing_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"found_missing.txt\",'w') as ofile :\n",
    "    for word in set(all_missing_words) :\n",
    "        ofile.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same with the 1M word corpus in `big.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_word_set = set()\n",
    "\n",
    "with open(\"big.txt\") as infile :\n",
    "    for row in infile :\n",
    "        row = row.lower()\n",
    "        row = \"\".join([ch for ch in row if not non_alpha.match(ch)])\n",
    "        \n",
    "        row = row.strip().split()\n",
    "        \n",
    "        words = [w for w in row if w not in big_word_set]\n",
    "                \n",
    "        big_word_set.update(set(words))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_missing_words = list()\n",
    "\n",
    "with open(input_file) as ifile :\n",
    "    for row in ifile :\n",
    "        row = row.lower()\n",
    "        row = \"\".join([ch for ch in row if not non_alpha.match(ch)])\n",
    "        \n",
    "        row = row.strip().split()\n",
    "        \n",
    "        missing_words = [w for w in row if w not in big_word_set]\n",
    "\n",
    "        all_missing_words.extend(missing_words)\n",
    "\n",
    "        \n",
    "        \n",
    "with open(\"found_missing_2.txt\",'w') as ofile :\n",
    "    for word in set(all_missing_words) :\n",
    "        ofile.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(big_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next exercise: write a function that takes a word and returns all deletions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_deletions(word) :\n",
    "    # given a word, return a list of every word that \n",
    "    # represents a single deletion from that word.\n",
    "    deletes = []\n",
    "    \n",
    "    for i in range(len(word)) :\n",
    "        deletes.append(str(word[:i] + word[(i+1):]))\n",
    "        \n",
    "        \n",
    "    return(deletes)\n",
    "\n",
    "print(get_deletions(\"test\"))\n",
    "print(get_deletions(\"longer\"))\n",
    "print(get_deletions(\"sesquipedalian\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now some code related to the stuff we talked about in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "word = \"monkey\"\n",
    "splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "inserts    = [L + c + R               for L, R in splits for c in letters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replaces[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "known(edits1(\"monkey\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WORDS.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correction(\"tasts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correction(\"thew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[a for a in WORDS if \"tk\" in a.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Running over Twitter Descriptions\n",
    "\n",
    "1. Run the spell checker over the first 1000 descriptions from one of your Twitter files. \n",
    "\n",
    "1. Print the misspelled words along with their correction. \n",
    "\n",
    "1. How many seem to be legit misspellings?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the twitter data\n",
    "file_location = \"C:\\\\Users\\\\jchan\\\\Dropbox\\\\Teaching\\\\2017_Spring\\\\UnstructuredData\\\\PreWork\\\\\"\n",
    "file_name = \"20170305_GeneralMills_followers.txt\"\n",
    "\n",
    "descs = []\n",
    "with open(file_location + file_name,'r') as ifile :\n",
    "    next(ifile)\n",
    "    for idx, line in enumerate(ifile.readlines()) :\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        \n",
    "        # spot 6 has the description\n",
    "        if len(line) >= 7 : # sometimes we don't have descriptions\n",
    "            descs.extend(line[6].split())\n",
    "            \n",
    "        if idx > 250 :\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misspell_count = 0\n",
    "\n",
    "non_alpha = re.compile(r\"[^a-z]\")\n",
    "\n",
    "for idx, word in enumerate(descs) :\n",
    "    word = word.lower()\n",
    "    word = non_alpha.sub(\"\",word)\n",
    "    \n",
    "    if word : # get rid of blanks\n",
    "        c = correction(word)\n",
    "        if c != word :\n",
    "            misspell_count += 1\n",
    "            print(\" : \".join([word,c]))\n",
    "\n",
    "#        if idx > 100 :\n",
    "#            break\n",
    "print(\"Found \" + str(misspell_count) + \" misspellings.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
