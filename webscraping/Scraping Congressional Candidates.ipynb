{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Congressional Candidates\n",
    "\n",
    "In this notebook, we'll scrape some congressional candidates' websites and store those results. This is really just scratching the surface of web scraping, but it'll orient you to the process. \n",
    "\n",
    "There are two main libraries you need to know about: `requests` and `BeautifulSoup` (actually called `bs4`). \n",
    "\n",
    "The [`requests`](http://docs.python-requests.org/en/master/#) library is a pretty delightful way to ask for information from web servers. I'd urge you to walk through the [quickstart guide](http://docs.python-requests.org/en/master/user/quickstart/) at your leisure to understand what's possible. \n",
    "\n",
    "[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) is the Python standard for parsing HTML. We don't have time to do a deep dive on HTML (nor am I really the person to teach it), so we'll pick and choose our pieces to use. But there's a ton there and, really, anything is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from random import sample, seed\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll grab some of our congressional candidate data. Let's read it in. For now we'll just grab some names and websites, but obviously there's a lot more data in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "congressional_file = \"partial_candidate_set.txt\"\n",
    "\n",
    "name_website = dict()\n",
    "\n",
    "with open(congressional_file,'r') as input_file :\n",
    "    next(input_file)\n",
    "    \n",
    "    for line in input_file.readlines() :\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        \n",
    "        # name is in the 7th spot, website in the 9th\n",
    "        # The strips make sure we don't get extra spaces, which \"name\" seems\n",
    "        # to have\n",
    "        name_website[line[6].strip()] = line[8].strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of these and see if they're right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx, name in enumerate(name_website) :\n",
    "    print(\"For {} we have {} as their website.\".format(name,name_website[name]))\n",
    "    \n",
    "    if idx == 4 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like some people have gotten incumbent's house website. That's not right, but let's see how often it happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for name, site in name_website.items() :\n",
    "    # many of the websites end in \"/\". Let's\n",
    "    # strip those off the end. \n",
    "    site = site.strip(\"/\")\n",
    "    \n",
    "    # Usually I delete comments like the code below, but I \n",
    "    # thought maybe you'd like to see how I actually do the work. \n",
    "    # The next two lines are the ones I used to figure out \n",
    "    # how to do work the \"slices\" of the string to properly get \n",
    "    # the .gov at the end of the string\n",
    "#    print(site[-4:])\n",
    "#    break\n",
    "    if site[-4:] == \".gov\" :\n",
    "        print(\"Oops, for {} we have {}\".format(name,site))\n",
    "        count += 1\n",
    "        \n",
    "print(\"Damn, there were {} wrong websites.\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's going to be irksome.... Anyway, back to scraping. Let's randomly choose a website and go scrape it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed(20181001) # change your seed if you'd like a different candidate.\n",
    "this_site = sample(list(name_website.values()),k=1)\n",
    "print(this_site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've picked the website, let's pull it using the requests library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get(this_site[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what's in the request result, type `r.` and the tab key in a cell and you'll see all the options. Some very useful ones are `status_code`s, `headers`, `json`, and `text`. Take a look at all of these and see if you can figure out what we're getting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(r.status_code)\n",
    "print(r.headers)\n",
    "print(r.json)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the request basically has all the information coming back from the page. We're going to use a very tiny amount of it's functionality, pass that into BeautifulSoup and use the BeautifulSoup parser to get the raw text out. Parsing HTML is not easy, but we can follow [this stackoverflow answer](https://stackoverflow.com/questions/1936466/beautifulsoup-grab-visible-webpage-text) and get most of the way there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4.element import Comment\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(text_from_html(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slick. I definitely couldn't have figured that out in any reasonable amount of time.\n",
    "\n",
    "---\n",
    "\n",
    "Let's put all those pieces together and build a dictionary that has the candidate's name as the key and all the text for a candidates website as the value. This will require a trick to grab the name from our first dictionary. There are more efficient ways to do this, but this is fast enough for our purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed(20181002)\n",
    "\n",
    "name_to_text = dict()\n",
    "\n",
    "sites = sample(list(name_website.values()),k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for this_site in sites :\n",
    "    \n",
    "    # first, let's get the names\n",
    "    for name, site in name_website.items() :\n",
    "        if this_site == site :\n",
    "            this_name = name\n",
    "            break\n",
    "            \n",
    "    # Now request the website\n",
    "    r = requests.get(this_site)\n",
    "    \n",
    "    if r.status_code == requests.codes.ok : # check out the requests doc. What is this doing?\n",
    "        name_to_text[name] = text_from_html(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now what? Well, there are a lot of things that we could do. For instance, let's look at the ten most common words on each candidate's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, site_text in name_to_text.items() :\n",
    "    \n",
    "    # let's split the text on whitespace, cast everything\n",
    "    # to lowercase, and throw it in a counter.\n",
    "    clean_text = site_text.split()\n",
    "    clean_text = [item.lower() for item in clean_text]\n",
    "    \n",
    "    text_count = Counter(clean_text)\n",
    "    \n",
    "    print(\"Candidate: \" + name + \"\\n\")\n",
    "    print(text_count.most_common(10))\n",
    "    print(\"\\n\"*3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the counts. Some people apparently don't have a lot of text, so we're getting some \"most common words\" with just a couple of counts. Others have higher counts, but we're getting things like the candidates names and a bunch of \"stop words\". We haven't talked about those yet, but they're just very common words that typically don't represent much information. It's relatively easy to strip them out. I'll bring them in, then repeat the same code as above, removing stopwords.\n",
    "\n",
    "Note: if the cell below doesn't work for you, follow these steps.\n",
    "\n",
    "On a Mac: \n",
    "\n",
    "1. Open up a terminal window.\n",
    "1. Type the following `sudo pip install -U nltk`\n",
    "1. Test the installation by typing `python` (opens python in the console), then type `import nltk`\n",
    "\n",
    "On Windows:\n",
    "\n",
    "1. Open up a command window.\n",
    "1. Type the following `conda install -c anaconda nltk`\n",
    "1. Test the installation by typing `python` (opens python in the console), then type `import nltk`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for name, site_text in name_to_text.items() :\n",
    "    \n",
    "    # let's split the text on whitespace, cast everything\n",
    "    # to lowercase, and throw it in a counter.\n",
    "    clean_text = site_text.split()\n",
    "    clean_text = [item.lower() for item in clean_text]\n",
    "    \n",
    "    # remove the stopwords\n",
    "    clean_text = [word for word in clean_text if word not in stop_words]\n",
    "    \n",
    "    text_count = Counter(clean_text)\n",
    "    \n",
    "    print(\"Candidate: \" + name + \"\\n\")\n",
    "    print(text_count.most_common(10))\n",
    "    print(\"\\n\"*3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we're starting to see some more interesting patterns. Sure, most of the words seem candidate specific (names, districts, etc.), but the gist is there. Let's look across 25 candidates, count the most common words, and see if anything emerges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed(20181001)\n",
    "\n",
    "num_candidates = 25\n",
    "\n",
    "# an empty list to hold all the words\n",
    "candidate_words = []\n",
    "\n",
    "sites = sample(list(name_website.values()),k=num_candidates)\n",
    "\n",
    "for this_site in sites :\n",
    "                \n",
    "    # Now request the website\n",
    "    r = requests.get(this_site)\n",
    "    \n",
    "    if r.status_code == requests.codes.ok : # check out the requests doc. What is this doing?\n",
    "        site_text = text_from_html(r.text)\n",
    "        \n",
    "        clean_text = site_text.split()\n",
    "        clean_text = [item.lower() for item in clean_text]\n",
    "        clean_text = [word for word in clean_text if word not in stop_words]\n",
    "\n",
    "        candidate_words.extend(clean_text)\n",
    "        \n",
    "print(\"Grabbed all the sites.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Counter(candidate_words).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what you're seeing here, what are the next things you might do to clean up this data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now it's time for you to do some original work. Write some code that does the following:\n",
    "\n",
    "1. Randomly select 10 congressional websites.\n",
    "1. Pull the text for the site, using the same tricks we did: cast to lowercase, clean out stopwords. Bonus: do the things you thought of in the previous question. For\n",
    "1. Build a dictionary with the following form: `my_dict[party][gender][word] = count`. Note that you'll have to go back to the original file to get party and gender associated with the url.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start by getting the mapping of url to party and gender    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now let's select 10 URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build your results dictionary that will have the form\n",
    "# results[party][gender][word] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and now fill it in!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
